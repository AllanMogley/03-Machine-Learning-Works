{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d0fa46a8",
      "metadata": {
        "id": "d0fa46a8"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/image-retrieval/cnn/cifar10.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b325e409-0b8c-4b80-a936-32b2b7002d68",
      "metadata": {
        "id": "b325e409-0b8c-4b80-a936-32b2b7002d68"
      },
      "source": [
        "### Generate Image Embeddings Using CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "001eb1ad-7172-4308-a87b-fd9c2ddb29aa",
      "metadata": {
        "id": "001eb1ad-7172-4308-a87b-fd9c2ddb29aa"
      },
      "source": [
        "In this section, we will use PyTorch to build a Convolutional Neural Network from scratch  that generates image embeddings.✔✔✔"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to a Cloud TPU which more powerfuls than a GPU\n",
        "# Make sure to select TPU from Edit > Notebook settings > Hardware accelerator\n",
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR']"
      ],
      "metadata": {
        "id": "6qx80CznSWWA"
      },
      "id": "6qx80CznSWWA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing PyTorch/XLA\n",
        "The PyTorch/XLA package lets PyTorch connect to Cloud TPUs. (It's named PyTorch/XLA, not PyTorch/TPU, because XLA is the name of the TPU compiler.) In particular, PyTorch/XLA makes TPU cores available as PyTorch devices. This lets PyTorch create and manipulate tensors on TPUs."
      ],
      "metadata": {
        "id": "Qyk7jhndTPFC"
      },
      "id": "Qyk7jhndTPFC"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "SpkWCzNGTb0-"
      },
      "id": "SpkWCzNGTb0-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating and Manipulating Tensors on TPUs\n",
        "\n",
        "PyTorch uses Cloud TPUs just like it uses CPU or CUDA devices, as the next few cells will show. Each core of a Cloud TPU is treated as a different PyTorch  device."
      ],
      "metadata": {
        "id": "os6jOAiQTqxV"
      },
      "id": "os6jOAiQTqxV"
    },
    {
      "cell_type": "code",
      "source": [
        "# imports pytorch\n",
        "import torch\n",
        "\n",
        "# imports the torch_xla package\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "\n",
        "# Creates a random tensor on xla:1 (a Cloud TPU core)\n",
        "device = xm.xla_device()"
      ],
      "metadata": {
        "id": "_Nxll__dTy1X"
      },
      "id": "_Nxll__dTy1X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2271958e-cf71-4df8-a3b8-a736dd9c98f7",
      "metadata": {
        "id": "2271958e-cf71-4df8-a3b8-a736dd9c98f7"
      },
      "source": [
        "#### Data Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b5121d0-9e68-4c12-98c5-3132bac48b24",
      "metadata": {
        "id": "0b5121d0-9e68-4c12-98c5-3132bac48b24"
      },
      "source": [
        "Before starting, below are some libraries we would need to build the convolutional network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "747eefd9-2d8d-4b1e-8f04-6e1f62b603e7",
      "metadata": {
        "id": "747eefd9-2d8d-4b1e-8f04-6e1f62b603e7"
      },
      "outputs": [],
      "source": [
        "# Load in relevant libraries, and alias where appropriate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5685d59c-987a-4c9d-af1b-8a9be1ca505b",
      "metadata": {
        "id": "5685d59c-987a-4c9d-af1b-8a9be1ca505b"
      },
      "source": [
        "Given we will be working with a reasonable amount of data, we might want to process it on GPU rather than CPU. To do that, we need to transfer the data from CPU to GPU using ```torch.device```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50974897-8297-49ed-a3a4-ae09d704313f",
      "metadata": {
        "id": "50974897-8297-49ed-a3a4-ae09d704313f"
      },
      "outputs": [],
      "source": [
        "# Device will determine whether to run the training on GPU or CPU.\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b4d1a1a-c55f-40ce-a4df-6ee38875b45d",
      "metadata": {
        "id": "3b4d1a1a-c55f-40ce-a4df-6ee38875b45d"
      },
      "source": [
        "The first step in building our network is downloading and initializing the dataset.\n",
        "\n",
        "The dataset we are downloading is *CIFAR-10* from HuggingFace.\n",
        "\n",
        "We will first download the training dataset by setting ```split = 'train'```, and the testing dataset after by setting ''split = 'test'```. While the training dataset includes 50,000 images divided into 10 classes, the test dataset includes 10,000 images divided into 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install datasets library\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "2SyyLvchWqvB"
      },
      "id": "2SyyLvchWqvB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87510a3c-9382-4a39-a39b-ded727bf6f1d",
      "metadata": {
        "id": "87510a3c-9382-4a39-a39b-ded727bf6f1d"
      },
      "outputs": [],
      "source": [
        "# import CIFAR-10 dataset from HuggingFace\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset_train = load_dataset(\n",
        "    'cifar10',\n",
        "    split='train[:1%]', # 1% training dataset\n",
        "    # split='train', # training dataset\n",
        "    ignore_verifications=True  # set to True if seeing splits Error\n",
        ")\n",
        "\n",
        "dataset_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df80571-3e94-492c-8750-898464f7092c",
      "metadata": {
        "id": "8df80571-3e94-492c-8750-898464f7092c"
      },
      "outputs": [],
      "source": [
        "# check how many labels/number of classes\n",
        "num_classes = len(set(dataset_train['label']))\n",
        "num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5cc6158-181b-4790-92b2-073670bfc975",
      "metadata": {
        "id": "a5cc6158-181b-4790-92b2-073670bfc975"
      },
      "outputs": [],
      "source": [
        "# let's view the image (it's very small)\n",
        "dataset_train[0]['img']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba410153-e667-4e75-953e-fa3969568139",
      "metadata": {
        "id": "ba410153-e667-4e75-953e-fa3969568139"
      },
      "outputs": [],
      "source": [
        "type(dataset_train[0]['img'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ffafa0e-7189-420f-9a9a-01fe33ed195a",
      "metadata": {
        "id": "9ffafa0e-7189-420f-9a9a-01fe33ed195a"
      },
      "source": [
        "Now pull in the test set (that we will use as a validation set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c419b9b4-13a1-4a21-8dfd-7d083f893220",
      "metadata": {
        "id": "c419b9b4-13a1-4a21-8dfd-7d083f893220"
      },
      "outputs": [],
      "source": [
        "dataset_val = load_dataset(\n",
        "    'cifar10',\n",
        "    split='test[:1%]', # 10% test dataset\n",
        "    # split='test', # test dataset\n",
        "    ignore_verifications=True  # set to True if seeing splits Error\n",
        ")\n",
        "\n",
        "dataset_val"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40ad0341-9989-4ddc-88a7-4ee31d9136d3",
      "metadata": {
        "id": "40ad0341-9989-4ddc-88a7-4ee31d9136d3"
      },
      "source": [
        "Most convolutional neural networks are designed so that they can only accept images of a fixed size. It's common practice to overcome this by reshaping the input images.\n",
        "\n",
        "We chose to reshape the image to 32 pixels - see ```img_size```.\n",
        "\n",
        "In addition, PyTorch uses tensors.\n",
        "\n",
        "Therefore, we will transform the data using the ```transforms.Compose``` method, and save the reshaped tensors in the ```preprocess``` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4d7c395-5424-4296-9b9b-f78ee9d08695",
      "metadata": {
        "id": "a4d7c395-5424-4296-9b9b-f78ee9d08695"
      },
      "outputs": [],
      "source": [
        "# image size\n",
        "img_size = 32\n",
        "\n",
        "# preprocess variable, to be used ahead\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((img_size,img_size)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df4d64ee-c5e5-4dfe-829c-35b0080c1ea0",
      "metadata": {
        "id": "df4d64ee-c5e5-4dfe-829c-35b0080c1ea0"
      },
      "source": [
        "We can now use the ```preprocess``` variable on the dataset with the following loop. However, when we build a cnn, we should ensure every image has the same number of input channels (or depth). This is because the input dimension to the cnn cannot change.\n",
        "\n",
        "If we have RGB images, the depth will be $3$, with one channel for each color (red, green, and blue). If we have grayscale images, the depth will be $1$.\n",
        "\n",
        "In case the dataset contains images with different number of channels, we should convert them from grayscale to RGB (or the other way round). When an image is in grayscale, its ```mode``` is `L`. Otherwise, the mode is `RGB`.\n",
        "\n",
        "We then converted the images to RGB and preprocessed them in the same loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d55444c-60ec-4913-9771-93db4824b91d",
      "metadata": {
        "id": "5d55444c-60ec-4913-9771-93db4824b91d"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "inputs_train = []\n",
        "\n",
        "for record in tqdm(dataset_train):\n",
        "    image = record['img']\n",
        "    label = record['label']\n",
        "\n",
        "    # convert from grayscale to RGB\n",
        "    if image.mode == 'L':\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "    # prepocessing\n",
        "    input_tensor = preprocess(image)\n",
        "\n",
        "    # append to batch list\n",
        "    inputs_train.append([input_tensor, label])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eaf6075",
      "metadata": {
        "id": "3eaf6075"
      },
      "outputs": [],
      "source": [
        "print(len(inputs_train), inputs_train[0][0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34234565-ff48-4333-8070-2786f0a6873d",
      "metadata": {
        "id": "34234565-ff48-4333-8070-2786f0a6873d"
      },
      "source": [
        "Below we can see the result from the transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69c256c0-8607-4d30-81ee-c7589e181eba",
      "metadata": {
        "id": "69c256c0-8607-4d30-81ee-c7589e181eba"
      },
      "outputs": [],
      "source": [
        "inputs_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4daa892-7cad-4548-9cee-babf173ae32d",
      "metadata": {
        "id": "f4daa892-7cad-4548-9cee-babf173ae32d"
      },
      "source": [
        "The tensors are normalized to a \\[0, 1\\] range by the `preprocess` pipeline thanks to the `transforms.ToTensor()` function. We need to modify this normalization slightly to fit this specific dataset. To do this, we need the mean and standard deviation values for each of the RGB channels across all images.\n",
        "\n",
        "We calculate that like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a0a938f-49aa-4daf-a018-e33ae705308e",
      "metadata": {
        "id": "1a0a938f-49aa-4daf-a018-e33ae705308e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# calculate mean and std of images, first start by choosing a random sample of tensors\n",
        "idx = np.random.randint(0, len(inputs_train), 512)\n",
        "idx.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7346e4a6-1dce-4e25-9f1c-b50841506947",
      "metadata": {
        "id": "7346e4a6-1dce-4e25-9f1c-b50841506947"
      },
      "outputs": [],
      "source": [
        "# then we concatenate this subset of image tensors\n",
        "tensors = torch.concat([inputs_train[i][0] for i in idx], axis=1)\n",
        "tensors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0ac48ca-28d6-406b-bdcc-e90ec3cfa068",
      "metadata": {
        "id": "a0ac48ca-28d6-406b-bdcc-e90ec3cfa068"
      },
      "outputs": [],
      "source": [
        "# merge all values into single 3-channel vector\n",
        "tensors = tensors.swapaxes(0, 1).reshape(3, -1).T\n",
        "tensors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19c3a7f7-a4a5-4170-9809-118bbf0de5ad",
      "metadata": {
        "id": "19c3a7f7-a4a5-4170-9809-118bbf0de5ad"
      },
      "outputs": [],
      "source": [
        "mean = torch.mean(tensors, axis=0)\n",
        "mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a1552eb-38ca-4022-95e7-48290efaa8e4",
      "metadata": {
        "id": "4a1552eb-38ca-4022-95e7-48290efaa8e4"
      },
      "outputs": [],
      "source": [
        "std = torch.std(tensors, axis=0)\n",
        "std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eb0368f-23c4-4f36-a6a2-14438fdec124",
      "metadata": {
        "id": "5eb0368f-23c4-4f36-a6a2-14438fdec124"
      },
      "outputs": [],
      "source": [
        "del tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f6d4ae1-69a6-4f98-b457-f2c43093bb38",
      "metadata": {
        "id": "9f6d4ae1-69a6-4f98-b457-f2c43093bb38"
      },
      "source": [
        "Now we use the `mean` $[0.4670, 0.4735, 0.4662]$ and `std` $[0.2496, 0.2489, 0.2521]$ to normalize via another preprocessing step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81cfbe6a-05fc-40c3-9136-dcecc7c36840",
      "metadata": {
        "id": "81cfbe6a-05fc-40c3-9136-dcecc7c36840"
      },
      "outputs": [],
      "source": [
        "preprocess = transforms.Compose([transforms.Normalize(mean=mean, std=std)])\n",
        "\n",
        "for i in tqdm(range(len(inputs_train))):\n",
        "    # prepocessing\n",
        "    input_tensor = preprocess(inputs_train[i][0])\n",
        "    inputs_train[i][0] = input_tensor  # replace with normalized tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1c55406-6f60-427e-b9dc-1f040d49c286",
      "metadata": {
        "id": "e1c55406-6f60-427e-b9dc-1f040d49c286"
      },
      "source": [
        "We can merge the preprocessing steps as follows,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88dd5eb8-33cb-48b6-9021-b979ec6ac102",
      "metadata": {
        "id": "88dd5eb8-33cb-48b6-9021-b979ec6ac102"
      },
      "outputs": [],
      "source": [
        "# merge the two preprocessing steps from before\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((img_size,img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0692ef00-7f2a-4917-9c90-21cc09bc8588",
      "metadata": {
        "id": "0692ef00-7f2a-4917-9c90-21cc09bc8588"
      },
      "source": [
        "and we can apply the same transformation to the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "931fa393-df08-4d54-95d6-0e2b4047d29c",
      "metadata": {
        "id": "931fa393-df08-4d54-95d6-0e2b4047d29c"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "inputs_val = []\n",
        "i = 0\n",
        "for record in tqdm(dataset_val):\n",
        "    image = record['img']\n",
        "    label = record['label']\n",
        "\n",
        "    # convert from grayscale to RBG\n",
        "    if image.mode == 'L':\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "    # prepocessing\n",
        "    input_tensor = preprocess(image)\n",
        "    inputs_val.append((input_tensor, label)) # append to batch list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b242ba-7621-4eae-a16c-b0d52a1e8418",
      "metadata": {
        "id": "28b242ba-7621-4eae-a16c-b0d52a1e8418"
      },
      "source": [
        "Given the amount of data, we would improve the efficiency of our model by running it in batches. We can set the batch size to $64$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1391456a-1ba6-42ab-9782-d1758e1fc849",
      "metadata": {
        "id": "1391456a-1ba6-42ab-9782-d1758e1fc849"
      },
      "outputs": [],
      "source": [
        "# define batch size\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b054f679-71f8-439c-9d29-6c63ae876d94",
      "metadata": {
        "id": "b054f679-71f8-439c-9d29-6c63ae876d94"
      },
      "source": [
        "We can then use ```DataLoader``` to split both the training and validation dataset into shuffled batches. Shuffle helps prevent model overfitting by ensuring that batches are more representative of the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a68ebd-6fe9-4e00-8471-4ff5174277ff",
      "metadata": {
        "id": "a1a68ebd-6fe9-4e00-8471-4ff5174277ff"
      },
      "outputs": [],
      "source": [
        "dloader_train = torch.utils.data.DataLoader(\n",
        "    inputs_train, batch_size=batch_size, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d4f6f12-de4c-4f2b-948b-e64466dbcb2b",
      "metadata": {
        "id": "8d4f6f12-de4c-4f2b-948b-e64466dbcb2b"
      },
      "outputs": [],
      "source": [
        "dloader_val = torch.utils.data.DataLoader(\n",
        "    inputs_val, batch_size=batch_size, shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a694096-4a99-49a5-9fae-76c826282ee6",
      "metadata": {
        "id": "4a694096-4a99-49a5-9fae-76c826282ee6"
      },
      "source": [
        "#### Building the CNN's Structure/Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09ef12b6-9cf6-4345-814b-05f231e2b5a6",
      "metadata": {
        "id": "09ef12b6-9cf6-4345-814b-05f231e2b5a6"
      },
      "source": [
        "We can now start building our cnn by generating the `ConvNeuralNet` class and the `forward` function. The `ConvNeuralNet` class defines the elements inside our network. The `forward` function establishes their order:\n",
        "\n",
        "- The first two blocks of layers are composed of a convolutional layer followed by a max-pooling layer. After each convolutional layer, we added non-linearity using the ReLU activation function.\n",
        "\n",
        "- The following two blocks of layers are composed of a convolutional layer followed by the ReLU activation function.\n",
        "\n",
        "- We then have another block composed of one convolutional layer, followed by the ReLU, followed by the max pooling layer.\n",
        "\n",
        "At each block, the images are downsampled by the max-pooling layer. Contrary, the number of channels from one layer to another increased from $3$ to $64$, to $192$, ..., to $256$. As we learned before, deeper layers have larger receptive fields and generally detect more specific and complex features, such as ears, eyes, or even human faces and dogs. The chosen filter (or kernel) size is either $3$ or $4$. This is a common choice - having a smaller filter allows the network to better generalize. Padding is $1$ pixel on each layer.\n",
        "\n",
        "- In the end, we have three fully-connected layers. The first two are activated using the ReLU, and the last one uses the Softmax.\n",
        "\n",
        "Note that we added `dropout`. It randomly zeroes (or \"drops out\") some of the elements of the input tensor with a probability $p = 25\\%$. This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper Improving neural networks by preventing co-adaptation of feature detectors [1].\n",
        "\n",
        "\n",
        "We can represent our network as follows:\n",
        "\n",
        "[image]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d41b3a3d-34fc-42f2-8fd1-fd1c4cd09b22",
      "metadata": {
        "id": "d41b3a3d-34fc-42f2-8fd1-fd1c4cd09b22"
      },
      "outputs": [],
      "source": [
        "# creating a CNN class\n",
        "class ConvNeuralNet(nn.Module):\n",
        "\t#  determine what layers and their order in CNN object\n",
        "    def __init__(self, num_classes):\n",
        "        super(ConvNeuralNet, self).__init__()\n",
        "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=4, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "        self.conv_layer3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.conv_layer4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "        self.conv_layer5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.max_pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "        self.dropout6 = nn.Dropout(p=0.5)\n",
        "        self.fc6 = nn.Linear(1024, 512)\n",
        "        self.relu6 = nn.ReLU()\n",
        "        self.dropout7 = nn.Dropout(p=0.5)\n",
        "        self.fc7 = nn.Linear(512, 256)\n",
        "        self.relu7 = nn.ReLU()\n",
        "        self.fc8 = nn.Linear(256, num_classes)\n",
        "\n",
        "    # progresses data across layers\n",
        "    def forward(self, x):\n",
        "        out = self.conv_layer1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.max_pool1(out)\n",
        "\n",
        "        out = self.conv_layer2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.max_pool2(out)\n",
        "\n",
        "        out = self.conv_layer3(out)\n",
        "        out = self.relu3(out)\n",
        "\n",
        "        out = self.conv_layer4(out)\n",
        "        out = self.relu4(out)\n",
        "\n",
        "        out = self.conv_layer5(out)\n",
        "        out = self.relu5(out)\n",
        "        out = self.max_pool5(out)\n",
        "\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "\n",
        "        out = self.dropout6(out)\n",
        "        out = self.fc6(out)\n",
        "        out = self.relu6(out)\n",
        "\n",
        "        out = self.dropout7(out)\n",
        "        out = self.fc7(out)\n",
        "        out = self.relu7(out)\n",
        "\n",
        "        out = self.fc8(out)  # final logits\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "202dbd79-0a54-4a06-84c3-036b9b4b7550",
      "metadata": {
        "id": "202dbd79-0a54-4a06-84c3-036b9b4b7550"
      },
      "source": [
        "After setting the architecture of our network, we can define the loss function, which is calculated after the forward propagation. We will then define the optimization methodology to run the backpropagation.\n",
        "\n",
        "The loss function used is cross-entropy.\n",
        "\n",
        "The model optimization is run using the stochastic gradient descent (SGD) method with a learning rate equal to $0.008$. Training under SGD, the network is forced to learn to extract features from the image that minimize the loss for extracting features that are the most useful for classifying or recognising images.\n",
        "The learning rate can be chosen randomly by running the model using different values and selecting the one that gives the best performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f37c0a86-e5bb-466c-b7d4-714d731dcaa3",
      "metadata": {
        "id": "f37c0a86-e5bb-466c-b7d4-714d731dcaa3"
      },
      "outputs": [],
      "source": [
        "# set the model to device\n",
        "model = ConvNeuralNet(num_classes).to(device)\n",
        "\n",
        "# set Loss function with criterion\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "# set learning rate\n",
        "lr = 0.008  # 0.0001\n",
        "\n",
        "# set optimizer with optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "#total_step = len(dloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ffd8888-9195-4463-aa83-aa93ed4a585d",
      "metadata": {
        "id": "4ffd8888-9195-4463-aa83-aa93ed4a585d"
      },
      "source": [
        "We are now ready to train our model with the forward and the backward propagation in batches. We are now using the `device` variable created above to move the data to GPU, if possible.\n",
        "\n",
        "We have set the times the learning algorithm will work through the entire training dataset (epochs) to 50. As per the learning rate, there is no rule for choosing this value. You can run the model many times with different values and choose the one with the sufficiently minimized error.\n",
        "\n",
        "It is also essential to not choose a number of `epochs` (or `lr`) that causes the model to overfit the training set. To check this, we include a pass through the validation set after each epoch, calculating the val-loss and val-accuracy. If we see the validation set performance degrades while train loss decreases, the model is likely overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3027a649-cd51-4c79-a672-2d6d372c1873",
      "metadata": {
        "tags": [],
        "id": "3027a649-cd51-4c79-a672-2d6d372c1873"
      },
      "outputs": [],
      "source": [
        "# train and validate the network\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\t# load in the data in batches\n",
        "    for i, (images, labels) in enumerate(dloader_train):\n",
        "        # move tensors to the configured device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # forward propagation\n",
        "        outputs = model(images)\n",
        "        loss = loss_func(outputs, labels)\n",
        "\n",
        "        # backward propagation and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # at end of epoch check validation loss and accuracy on validation set\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_val_loss = []\n",
        "        for images, labels in dloader_val:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            total += labels.size(0)\n",
        "            # calculate predictions\n",
        "            predicted = torch.argmax(outputs, dim=1)\n",
        "            # calculate actual values\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            # calculate the loss\n",
        "            all_val_loss.append(loss_func(outputs, labels).item())\n",
        "        # calculate val-loss\n",
        "        mean_val_loss = sum(all_val_loss) / len(all_val_loss)\n",
        "        # calculate val-accuracy\n",
        "        mean_val_acc = 100 * (correct / total)\n",
        "\n",
        "    print(\n",
        "        'Epoch [{}/{}], Loss: {:.4f}, Val-loss: {:.4f}, Val-acc: {:.1f}%'.format(\n",
        "            epoch+1, num_epochs, loss.item(), mean_val_loss, mean_val_acc\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c310d89-2083-482b-a9f4-bf59041de8c2",
      "metadata": {
        "id": "6c310d89-2083-482b-a9f4-bf59041de8c2"
      },
      "source": [
        "The network is now trained and tested! The network validation accuracy is $79.4\\%$. We can go ahead and save the model to file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05989bae",
      "metadata": {
        "id": "05989bae"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'cnn.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb707d56-7c01-47d1-a06d-cfb2bcac2e4c",
      "metadata": {
        "id": "fb707d56-7c01-47d1-a06d-cfb2bcac2e4c"
      },
      "source": [
        "---\n",
        "\n",
        "# Inference\n",
        "\n",
        "Now we will look at how to make predictions with the model. We start by loading the model from file:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ERULU-fDmas6"
      },
      "id": "ERULU-fDmas6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !dir\n",
        "%cd MyDrive/02 - Project\n",
        "\n"
      ],
      "metadata": {
        "id": "x9mgX-Ldm3Ld"
      },
      "id": "x9mgX-Ldm3Ld",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!dir"
      ],
      "metadata": {
        "id": "JglcgMf3nZc1"
      },
      "id": "JglcgMf3nZc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de3b2496",
      "metadata": {
        "id": "de3b2496"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "model = torch.load('cnn.pt')\n",
        "# switch to evaluation mode and device\n",
        "model.eval().to(device)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73553acc-9fb3-41e5-ba2a-dd575f58a14e",
      "metadata": {
        "id": "73553acc-9fb3-41e5-ba2a-dd575f58a14e"
      },
      "source": [
        "We will reinitialize the CIFAR-10 test set to similate a typical scenario where we would need to reload everything. Ideally we would not use the validation set as a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7463c3a9-33e5-4917-8f56-dbbbe55ab4af",
      "metadata": {
        "id": "7463c3a9-33e5-4917-8f56-dbbbe55ab4af"
      },
      "outputs": [],
      "source": [
        "# import CIFAR-10 dataset from HuggingFace\n",
        "from datasets import load_dataset\n",
        "\n",
        "data_test = load_dataset(\n",
        "    'cifar10',\n",
        "    split='test'  # test set\n",
        ")\n",
        "data_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b144865d-c66d-4121-8925-a84cdb6574f0",
      "metadata": {
        "id": "b144865d-c66d-4121-8925-a84cdb6574f0"
      },
      "outputs": [],
      "source": [
        "input_tensors = []\n",
        "for image in data_test['img'][:10]:\n",
        "    tensor = preprocess(image)\n",
        "    input_tensors.append(tensor.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d315e65-658b-4fa3-8c96-93eb9a05a8fe",
      "metadata": {
        "id": "2d315e65-658b-4fa3-8c96-93eb9a05a8fe"
      },
      "outputs": [],
      "source": [
        "# we have 10 tensors\n",
        "len(input_tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d12e0287-5d66-44a4-8a19-144fb5dba9c5",
      "metadata": {
        "id": "d12e0287-5d66-44a4-8a19-144fb5dba9c5"
      },
      "outputs": [],
      "source": [
        "# all 32x32 dimensional with 3 color channels\n",
        "input_tensors[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16ed9a7e-85cb-4c8b-9dd3-a878d2478437",
      "metadata": {
        "id": "16ed9a7e-85cb-4c8b-9dd3-a878d2478437"
      },
      "source": [
        "We stack these into a single tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "225dfd48-5a59-4b09-9e71-a0b7be377676",
      "metadata": {
        "id": "225dfd48-5a59-4b09-9e71-a0b7be377676"
      },
      "outputs": [],
      "source": [
        "# stack into a single tensor\n",
        "input_tensors = torch.stack(input_tensors)\n",
        "input_tensors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c1a6d62-96f1-4436-8175-7e98788f8aa8",
      "metadata": {
        "id": "7c1a6d62-96f1-4436-8175-7e98788f8aa8"
      },
      "outputs": [],
      "source": [
        "# process through model to get output logits\n",
        "outputs = model(input_tensors)\n",
        "# calculate predictions\n",
        "predicted = torch.argmax(outputs, dim=1)\n",
        "predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "476f030e-3da1-40ac-a935-1a2961d8da2a",
      "metadata": {
        "id": "476f030e-3da1-40ac-a935-1a2961d8da2a"
      },
      "outputs": [],
      "source": [
        "predicted.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b15f4b77-8670-4c2d-b8e5-9d19d6b94da5",
      "metadata": {
        "id": "b15f4b77-8670-4c2d-b8e5-9d19d6b94da5"
      },
      "outputs": [],
      "source": [
        "# here are the class names\n",
        "data_test.features['label'].names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9be8f514-150f-4700-98e7-7968fc54f6c7",
      "metadata": {
        "id": "9be8f514-150f-4700-98e7-7968fc54f6c7"
      },
      "outputs": [],
      "source": [
        "data_test[1]['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b4ebfdc-c24f-4e6f-9437-38a1694d0943",
      "metadata": {
        "id": "4b4ebfdc-c24f-4e6f-9437-38a1694d0943"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i, image in enumerate(data_test['img'][:10]):\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n",
        "    print(data_test.features['label'].names[predicted[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4aca325-39d3-4289-a616-3e09f7704590",
      "metadata": {
        "id": "a4aca325-39d3-4289-a616-3e09f7704590"
      },
      "source": [
        "Most of these predictions look correct, despite being very low resolution images."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f29b2684-2626-4027-9ecf-28779b17ac67",
      "metadata": {
        "id": "f29b2684-2626-4027-9ecf-28779b17ac67"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "common-cu110.m95",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cu110:m95"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "5fe10bf018ef3e697f9035d60bf60847932a12bface18908407fd371fe880db9"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}